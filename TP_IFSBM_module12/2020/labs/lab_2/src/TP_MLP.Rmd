---
title: "TP Multi Layer Perceptron"
author: "L. Verlingue & Y. Pradat"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: readable
    toc: true
    toc_float: true
    code_folding: show
  rmdformats::material:
    highlight: kate
---

# What you will do in this R Notebook

## 1.1 Introduction
This is the second day of your training in R for Machine Learning. You will now enter the wide, wild and exciting field of Deep Learning!

Neural Networks and Deep Learning have revolutionized the performance of many Machine Learning tasks: in image analysis, gaming and natural langage understanding, for example.

This notebook will introduce you to the basic functions of a simple type of neural network: a multilayer perceptron (MLP). 

I hope you'll enjoye this Notebook and that it can help you to adress your current and/or future challenges.

## 1.2 Main steps
After loading TCGA data, we will build and train a simple neural network.

## 1.3 In practice
The code is provided to you. You will just have to follow the instructions all along the Notebook.
I'll give you a little bit of explanation when needed. 

Pay attention: there will be some questions to answer in this notebook.

*look at the slides to warm up*

# 2. Functions

```{r}
source("../../lib/LoadcBioportal150119.R")

set.seed(1234)
```


## 2.1 Data
Select the genes you want to use.

```{r}
CGS<-read.csv("../data/CancerGeneCensusCOSMIC.csv",stringsAsFactors = F)
GENES<-CGS$Gene.Symbol[CGS$Hallmark=="Yes"]
print(paste("You have selected",length(GENES),"important cancer genes like:", paste(sample(GENES,20), collapse = ", "), "..."))
```

Dowload TCGA data from the LoadcBioportal that you know.

```{r}
# load TCGA data of lung adenocarcinoma and lung squamous cell carcinoma

# set the argument Organ 
TCGAdata<-LoadcBioportal(Genes = GENES, Organ = "luad|gbm", 
                         ClinicNeeded = T, MutNeeded = F, 
                         RNANeeded = T, NormalizeRNA = T, 
                         FunctionalAnnot = F, PDF = F, Tests=T)
STUDY<-TCGAdata$STUDY
print("Here are your patients' numbers:")
print(table(TCGAdata$CLINIC$study))
```

Warm up by printing a little part of the expression data you have loaded (not the full table!!!)
```{r}
#### YOUR CODE HERE ####

#### END CODE ####

```

## 2.2 Split the data

You are now familliar with splitting your data.

```{r}
TrainSplit=0.8
Train<-sample(seq(nrow(TCGAdata$CLINIC)),size = round(nrow(TCGAdata$CLINIC)*TrainSplit) )
Val<-seq(nrow(TCGAdata$CLINIC))[!seq(nrow(TCGAdata$CLINIC))%in%Train]
paste("Your training and validation cohorts have ",length(Train),"&", length(Val), "patients.")
```

### 2.2.1 Input data
You will be performing matrix calculation, so turn your data into matrices, and rpint its dimensions.
```{r}
X<-as.matrix(TCGAdata$EXP[Train,])
dim(X)
```

### 2.2.2 Output label
Idem for outputs that should be numerical.
```{r}
Y<-as.matrix(as.numeric(as.factor(TCGAdata$CLINIC$study[Train]))-1)
head(Y)
```

## 2.3 Design your model

We want to do a 3 layer MLP. Let's define the number of units per layer.
```{r}
# 3 layer MLP
L1=30
L2=15
L3=1
```

You want to know what happened? *look at the slides*

### 2.3.1 Init

Random initialisation of the weights for each layers. The following weight matrix are detailed, one by one. Dimension are *n,m* (N units layer L, N units layer L-1).
Weight matrix of layer input data to L1.
```{r}
InitW1<-matrix(rnorm(dim(X)[2]*L1),nrow = L1 , ncol = dim(X)[2] )
dim(InitW1)
InitB1<-rep(0,L1)
```

Weight matrix of layer L1 to L2.
```{r}
InitW2<-matrix(rnorm(L1*L2),nrow = L2, ncol = L1)
dim(InitW2)
InitB2<-rep(0,L2)
```


Your turn: initialize the weight matrix of layer L2 to L3.
```{r}
#### YOUR CODE HERE ####

#### END CODE ####

InitB3<-rep(0,L3)
```

Finaly, store your weight matrices and beta vectors in a list.
```{r}
W<-list(InitW1, InitW2, InitW3)
B<-list(InitB1, InitB2, InitB3)
```


### 2.3.2 Activation function
Set your activation function: here we have chosed sigmoid update.
```{r}
Sigmoid<-function(z){1/(1+exp(-z))}
```

### 2.3.3 Update function
Set your update function.
```{r}
Fw<-function(X,W,B){Sigmoid(X%*%t(W)+B)}
```



### 2.3.4 Foward propagation

#### step by step
1st layer
For the first layer: matrix multiplication of the L1 weights *InitW1* and input data *X*.
```{r}
# exemple
paste("Data dimensions :", paste( dim(X), collapse = ",") )
paste("WL1 dimensions :", paste( dim(InitW1), collapse = ",") )
A1<-Fw(X=X, W = W[[1]], B = B[[1]])
paste("L1 unit dimensions :", paste(dim(A1), collapse = ",") )
```

2nd layer
```{r}
A2<-Fw(X=A1, W = W[[2]], B = B[[2]])
dim(A2)
```

3rd layer
```{r}
Yhat<-Fw(X=A2, W = W[[3]], B = B[[3]])
dim(Yhat)
```

## 2.4 First predictions

### 2.4.1 Loss function
Define your cross entropy loss function
```{r}
Cost<-function(Y,Yhat){ -mean(Y*log(Yhat)+(1-Y)*log(1-Yhat)) }
```

Visualize the confusion matrix and the cost.
```{r}
table(Y,round(Yhat))
print(paste("Cost =",Cost(Y,Yhat)))
```


### 2.4.2 In a function
You can store these foward propagation into a function. It will be usefull to iterate.
```{r}
StepsFw<-function(X,W,B){
  A0<-X
   A1<-Fw(X=X, W = W[[1]], B = B[[1]])
   A2<-Fw(X=A1,W = W[[2]], B = B[[2]])
  A3<-Fw(X=A2,W = W[[3]], B = B[[3]])
  return(cache=list(A0=A0,A1=A1,A2=A2,A3=A3))
}
```

To run it, simply call the function as follow:
```{r}
cache<-StepsFw(X,W,B)
# the values of each units are stores in the list "cache".
# the predictions of the model are the values of the last layer.
Yhat<-cache[[length(cache)]]
```


### 2.4.1 Back propagation

As presented in the lecture, you have to compute the derivations of the loss given the parameters of the model. This uses the chain rule of calculus. Every step of the computational graph are derived step by step up to the parameters.
Several conventional steps are encoded in the following functions.
There is a trick to compute the derivate of the categorical cross entropy (logistic cost) given the value z (the results of the linear function before activation)!

#### derivate of cost (=loss): dZ/DL
This is the trick:
```{r}
derivCost<-function(Y,Yhat){ (Yhat - Y) }
```

#### derivate of sigmoid
```{r}
derivSig<-function(z){Sigmoid(z)*(1-Sigmoid(z))}
```

#### set learning rate
An important hyperparameter!
```{r}
lr=0.01
```

#### Function for a single step back
Stack together your derivates into a chain rule function.
```{r}
StepBack<-function(dX,W,cache,Y, Yhat, step=3){
  
  # For the first step back, you can use the derivation of the cost. 
  # If not, use the derivation of the sigmoid
  if(step==length(W)){
    # derivate the cost function
    dZ<-derivCost(Y, Yhat)
  } else {
    # derivate the previous layer value
    dZ<-as.matrix(derivSig(dX))
  }
  
  # Next, derivate up to the weights and biase terms 
  
  #dim(dZ);dim(cache[[step]])
  dW<-t(dZ)%*%cache[[step]]
  
  #dim(dW)
  db<-colMeans(dZ)
  #dim(db)
  
  # Compute the previous dX
  #dim(dZ);dim(W[[step]])
  dX <- dZ%*%W[[step]]
  #dim(dX)
  
  return(list(dZ=dZ,dW=dW,db=db,dX=dX))
}
```

Run your function for the layers in your neural net. A loop is necessary to 

```{r}

for(i in rev(seq(length(W)))){
  dcache<-StepBack(dX,W,cache,Y,Yhat,step = i)
  dim(dcache$dW)
  
  # update W and B
  W[[i]]<-W[[i]]-lr*dcache$dW
  B[[i]]<-B[[i]]-lr*dcache$db
  
  # update dX
  dX<-dcache$dX
  #dim(dX)
}
```


### 2.4.2 Results
```{r}
cache<-StepsFw(X,W,B)
table(Y,round(cache[[4]]))
print(paste("Cost =",Cost(Y,cache[[4]])))

```

#### iterate

```{r}
#turn a foward + backward pass into a function
train<-function(X,W,B,Y,lr=0.01,iteration=10){
  Cost<-vector()
  
  for(iter in seq(iteration)){
    
    cache<-StepsFw(X,W,B)
  #Yhat<-cache[[4]]
      # store cost
    Cost<-c(Cost,Cost(Y,cache[[length(cache)]]))
  
    #dX<-derivCost(Y, cache[[length(cache)]])
    
    for(i in rev(seq(length(W))) ){
    
    dcache<-StepBack(dX,W,cache,Y,Yhat = cache[[length(cache)]],step = i)
    #dim(dcache$dW)
    
    # update W and B
    W[[i]]<-W[[i]]-lr*dcache$dW
    B[[i]]<-B[[i]]-lr*dcache$db
    
    # update dX
    dX<-dcache$dX
    
    #dim(dX)
    }
    
  }
  return(list(Cost=Cost,W=W,B=B))
}

```

### 2.4.3 run train function
```{r}
history<-train(X,W,B,Y,lr=0.001,iteration=400)
```


### 2.4.4 check results
```{r}
plot(history$Cost,type = "l", ylab = "Cost", xlab = "iterations")
```


```{r}
cache<-StepsFw(X,history$W,history$B)
table(Y,round(cache[[4]]))
Cost(Y,cache[[4]])
```

```{r}
image(InitW1)
image(history$W[[1]])
```


---
